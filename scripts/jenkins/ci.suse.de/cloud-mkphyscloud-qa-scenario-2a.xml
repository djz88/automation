<?xml version="1.0" encoding="UTF-8"?>
<project>
  <actions/>
  <description>&lt;b&gt;This job is managed by JJB! Changes must be done in&#13;
&lt;a href='https://github.com/SUSE-Cloud/automation/tree/master/jenkins/ci.suse.de/'&gt;git&lt;/a&gt;&#13;
&lt;/b&gt;&#13;
&#13;
This job will redeploy scenario-2a:&#13;
  - total 7 nodes including admin node&#13;
  - database: default, postgresql&#13;
  - pacemaker: 3 clusters with 2 nodes (SBD)&#13;
  - keystone UUID&#13;
  - swift (allow public container, enable object versioning)&#13;
  - glance (storage swift)&#13;
  - nova: ssl, libvirt migration enabled, shared storage, kvm kernel samepage merging, 1 KVM&#13;
  - cinder: Local file&#13;
  - neutron: linuxbridge or OVS &#13;
  - manila: NetApp backend (we expect that a vserver 'cloud-manila-svm' is available on the 'netapp-n1-e0m.cloud.suse.de' server)&#13;
&#13;
Warning: It will wipe all machines!&#13;
&lt;!-- Managed by Jenkins Job Builder --&gt;</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <jenkins.model.BuildDiscarderProperty>
      <strategy class="hudson.tasks.LogRotator">
        <daysToKeep>-1</daysToKeep>
        <numToKeep>15</numToKeep>
        <artifactDaysToKeep>-1</artifactDaysToKeep>
        <artifactNumToKeep>-1</artifactNumToKeep>
      </strategy>
    </jenkins.model.BuildDiscarderProperty>
    <com.sonyericsson.jenkins.plugins.bfa.model.ScannerJobProperty plugin="build-failure-analyzer@">
      <doNotScan>false</doNotScan>
    </com.sonyericsson.jenkins.plugins.bfa.model.ScannerJobProperty>
    <com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty plugin="gitlab-plugin@">
      <gitLabConnection>https://gitlab.suse.de</gitLabConnection>
    </com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty>
    <org.jenkinsci.plugins.gitlablogo.GitlabLogoProperty plugin="gitlab-logo@">
      <repositoryName/>
    </org.jenkinsci.plugins.gitlablogo.GitlabLogoProperty>
    <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@">
      <autoRebuild>false</autoRebuild>
      <rebuildDisabled>false</rebuildDisabled>
    </com.sonyericsson.rebuild.RebuildSettings>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>hw_number</name>
          <description>Mandatory, name of the QA cloud server as integer</description>
          <defaultValue>2</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>admin_os</name>
          <description>Mandatory, admin node operating system version</description>
          <defaultValue>sles12sp2</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>shared_storage_ip</name>
          <description>Mandatory, shared storage server IP</description>
          <defaultValue>10.162.66.1</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>repo_owner</name>
          <description>Mandatory, automation repo owner/organization</description>
          <defaultValue>SUSE-Cloud</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>automation_repo</name>
          <description>Mandatory, automation repo URL</description>
          <defaultValue>git://github.com/$repo_owner/automation.git</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>branch</name>
          <description>Mandatory, automation repo branch</description>
          <defaultValue>master</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>netapp_server</name>
          <description>Mandatory, the name of the NetApp Storage backend server</description>
          <defaultValue>netapp-n1-e0m.cloud.suse.de</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>netapp_vserver</name>
          <description>Mandatory, the name of the NetApp Storage backend vserver</description>
          <defaultValue>cloud-manila-svm</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>tempest</name>
          <description>Optional, specify what tempest test(s) to run, e.g. smoke, smoke|full or smoke|defcore</description>
          <defaultValue>smoke</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>cct</name>
          <description>Optional, specify cct tests to run</description>
          <defaultValue>features:base</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>ssl_type</name>
          <description>Mandatory, set the SSL configuration for the cloud, available options: no-ssl, ssl-insecure, ssl</description>
          <defaultValue>no-ssl</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>cloud_version</name>
          <description>Mandatory, version of the cloud to be installed as integer</description>
          <defaultValue>7</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>scenario_file</name>
          <description>Scenario YAML file name</description>
          <defaultValue>qa-scenario-2a.yaml</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>cloudsource</name>
          <description/>
          <defaultValue>develcloud$cloud_version</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>TESTHEAD</name>
          <description>if non-empty, test latest version from Devel:Cloud:Staging</description>
          <defaultValue>1</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>hacloud</name>
          <description>By default we do not want HA configured and installed</description>
          <defaultValue>1</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>clusterconfig</name>
          <description>HA configuration for clusters. Make sense only if hacloud=1</description>
          <defaultValue>services=2,data=2,network=2</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>networkingplugin</name>
          <description>networking plugin to be used by Neutron. Available options are: openvswitch:gre, vlan, vxlan / linuxbridge:vlan
</description>
          <defaultValue>linuxbridge</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>networkingmode</name>
          <description>networking mode to be used by Neutron. Available options are gre, vlan, vxlan</description>
          <defaultValue>vlan</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>nodenumber</name>
          <description>Number of nodes to use; is scenario specific</description>
          <defaultValue>7</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>want_ipmi</name>
          <description>Boolean. If 'true' nodes will be rebooted via IPMI</description>
          <defaultValue>true</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>commands</name>
          <description>All the steps that needs to be completed to have cloud installed</description>
          <defaultValue>addupdaterepo prepareinstallcrowbar runupdate bootstrapcrowbar installcrowbar allocate waitcloud setup_aliases</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.TextParameterDefinition>
          <name>UPDATEREPOS</name>
          <description>Update repositories (one URL per line)</description>
          <defaultValue/>
        </hudson.model.TextParameterDefinition>
        <hudson.model.BooleanParameterDefinition>
          <name>UPDATEBEFOREINSTALL</name>
          <description>add update repos before crowbar install</description>
          <defaultValue>false</defaultValue>
        </hudson.model.BooleanParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
    <hudson.plugins.heavy__job.HeavyJobProperty plugin="heavy-job@1.1">
      <weight>1</weight>
    </hudson.plugins.heavy__job.HeavyJobProperty>
  </properties>
  <scm class="hudson.scm.NullSCM"/>
  <assignedNode>cloud-mkphyscloud-gate-qa</assignedNode>
  <canRoam>false</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
admin=crowbar$hw_number
cloud=qa$hw_number
netapp_server=$netapp_server
netapp_password=`cat /home/jenkins/passwords/netapp_password`

if [ ! -z "$UPDATEREPOS" ] ; then
  export UPDATEREPOS=${UPDATEREPOS//$'\n'/+}
fi

export artifacts_dir=$WORKSPACE/.artifacts
rm -rf $artifacts_dir
mkdir -p $artifacts_dir
touch $artifacts_dir/.ignore
  
# check that netapp backend server is available
ping -c 3 $netapp_server || ret=$?

if [ $ret != 0 ] ; then 
  echo "netapp server is unavailable!"
  exit 1
fi

# destroy the old admin VM if any and spawn a clean new admin VM afterwards
# /usr/local/sbin/freshadminvm
freshadminvm $admin $admin_os

sleep 100 # time for the admin VM to boot

# wipe out shared NFS that are being used by this deployment
# /usr/local/sbin/wipe_nfs_shares
ssh root@localhost "wipe_nfs_shares qa$hw_number"

# clone, fetch and update the automation repo
# /usr/local/sbin/update_automation
update_automation

# put variables needed during the build process into mkcloud.config file
env | grep -e networking -e libvirt -e cloud &gt; mkcloud.config

# copy scripts/ directory onto the admin  node in /root/scripts
scp -r ~/github.com/$repo_owner/automation/scripts mkcloud.config root@$admin:

# copy scenario file onto the admin node in /root/scenario.yml
scp ~/github.com/$repo_owner/automation/scripts/scenarios/cloud$cloud_version/qa/$ssl_type/$scenario_file \
    root@$admin:scenario.yml

ret=0

ssh root@$admin "
export cloud=$cloud ;
export hw_number=$hw_number ;
export UPDATEREPOS=$UPDATEREPOS ;
export UPDATEBEFOREINSTALL=$UPDATEBEFOREINSTALL ;
export TESTHEAD=$TESTHEAD ;
export cloudsource=$cloudsource ;
export nodenumber=$nodenumber ;
export hacloud=$hacloud ;
export clusterconfig=$clusterconfig ;
export shared_storage_ip=$shared_storage_ip ;
export want_node_aliases=controller=2,data=2,network=2,computekvm=1 ;
export want_node_roles=controller=2,storage=2,network=2,compute=1 ;
export scenario=\"/root/scenario.yml\" ;
export netapp_password=$netapp_password ;
export netapp_server=$netapp_server ;
export netapp_vserver=$netapp_vserver ;
export commands=\"$commands\" "' 

sed -i -e "s,##shared_nfs_for_database##,$shared_storage_ip:/var/$cloud/ha-database," scenario.yml
sed -i -e "s,##shared_nfs_for_rabbitmq##,$shared_storage_ip:/var/$cloud/ha-rabbitmq," scenario.yml
sed -i -e "s,##cinder-storage-shares##,$shared_storage_ip:/var/$cloud/cinder-storage," scenario.yml
sed -i -e "s,##netapp_password##,$netapp_password," scenario.yml
sed -i -e "s,##netapp_server##,$netapp_server," scenario.yml
sed -i -e "s,##netapp_vserver##,$netapp_vserver," scenario.yml
sed -i -e "s,##networkingplugin##,$networkingplugin," scenario.yml
sed -i -e "s,##networkingmode##,$networkingmode," scenario.yml

[ $UPDATEBEFOREINSTALL == "true" ] &amp;&amp; export updatesteps="addupdaterepo runupdate"

# first install the nodes and wait until the cloud is ready for further configuration steps
timeout --signal=ALRM 240m bash -x -c ". scripts/qa_crowbarsetup.sh ; onadmin_runlist $commands"
' || ret=$?

echo "mkphyscloud ret=$ret (before scenario and configuration)"

if [ $ret == "0" ]; then
  # ----- Prepare the SBD setup:

  cat &gt; /tmp/sbd_prepare_$admin &lt;&lt;EOSCRIPT
    # preparation of iSCSI
    zypper --gpg-auto-import-keys -p http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12_SP2/ --non-interactive install python-sh

    wget --no-check-certificate https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/iscsictl.py

    chmod +x scripts/iscsictl.py
    
    shared_storage=10.162.64.10

    ./scripts/iscsictl.py --service initiator --target_host $shared_storage --host controller1 --no-key --id id-qa02
    ./scripts/iscsictl.py --service initiator --target_host $shared_storage --host controller2 --no-key --id id-qa02
    ./scripts/iscsictl.py --service initiator --target_host $shared_storage --host data1       --no-key --id id-qa02
    ./scripts/iscsictl.py --service initiator --target_host $shared_storage --host data2       --no-key --id id-qa02
    ./scripts/iscsictl.py --service initiator --target_host $shared_storage --host network1    --no-key --id id-qa02
    ./scripts/iscsictl.py --service initiator --target_host $shared_storage --host network2    --no-key --id id-qa02

    # preparation of SBD for services nodes
    SBD_DEV_SERVICES=\$(ssh controller1 echo '/dev/disk/by-id/scsi-\$(lsscsi -i | grep LIO | tr -s " " |cut -d " " -f7)')
    ssh controller1 "zypper --non-interactive install sbd; sbd -d \$SBD_DEV_SERVICES create"
    ssh controller2 "zypper --non-interactive install sbd"
    # take scenario yaml file and replace placeholders
    sed -i "s|@@sbd_device_services@@|\${SBD_DEV_SERVICES}|g" scenario.yml

    # preparation of SBD for data nodes
    SBD_DEV_DATA=\$(ssh data1 echo '/dev/disk/by-id/scsi-\$(lsscsi -i | grep LIO | tr -s " " |cut -d " " -f7)')
    ssh data1 "zypper --non-interactive install sbd; sbd -d \$SBD_DEV_DATA create"
    ssh data2 "zypper --non-interactive install sbd"
    # take scenario yaml file and replace placeholders
    sed -i "s|@@sbd_device_data@@|\${SBD_DEV_DATA}|g" scenario.yml

    # preparation of SBD for network nodes
    SBD_DEV_NETWORK=\$(ssh network1 echo '/dev/disk/by-id/scsi-\$(lsscsi -i | grep LIO | tr -s " " |cut -d " " -f7)')
    ssh network1 "zypper --non-interactive install sbd; sbd -d \$SBD_DEV_NETWORK create"
    ssh network2 "zypper --non-interactive install sbd"
    # take scenario yaml file and replace placeholders
    sed -i "s|@@sbd_device_network@@|\${SBD_DEV_NETWORK}|g" scenario.yml

    # watchdog configuration
    ssh controller1 "modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf"
    ssh controller2 "modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf"

    ssh data1 "modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf"
    ssh data2 "modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf"

    ssh network1 "modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf"
    ssh network2 "modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf"

    # ----- End of SBD
EOSCRIPT

  chmod +x /tmp/sbd_prepare_$admin

  scp /tmp/sbd_prepare_$admin root@$admin:sbd_prepare

  # Rerun chef-client
  ssh roott@$admin 'chef-client'

  # Check if zypper is used by other application
  ssh root@$admin '
    source scripts/qa_crowbarsetup.sh;
    for node in $(crowbar machines aliases | grep -E "controller|data|network" | grep -oE "^.*[[:space:]]"); do
      wait_for 20 10 "ssh $node \"zypper refresh\" "
    done
  '

  ssh root@$admin  "
  export cloud=$cloud ;
  export TESTHEAD=$TESTHEAD ;
  export cloudsource=$cloudsource ;
  export nodenumber=$nodenumber ;
  export hacloud=$hacloud ;
  export clusterconfig=$clusterconfig ;
  export want_ceph=1 ;
  export cephvolumenumber=0;
  export scenario=scenario.yml "'

  source scripts/qa_crowbarsetup.sh
  ./sbd_prepare

  crowbar batch --timeout 2400 build scenario.yml' || ret=$?

  if [ $ret != 0 ] ; then
    ssh root@$admin '
    set -x
    for node in $(crowbar machines list | grep ^d) ; do
      (
      echo "Collecting supportconfig from $node"
      timeout 400 ssh $node supportconfig | wc
      timeout 300 scp $node:/var/log/\*tbz /var/log/
      )&amp;
    done
    timeout 500 supportconfig | wc &amp;
    wait
    '

    scp root@$admin:/var/log/*tbz $artifacts_dir/
  fi &gt;&amp;2
fi

exit $ret
</command>
    </hudson.tasks.Shell>
    <hudson.plugins.parameterizedtrigger.TriggerBuilder plugin="parameterized-trigger@">
      <configs>
        <hudson.plugins.parameterizedtrigger.BlockableBuildTriggerConfig>
          <configs>
            <hudson.plugins.parameterizedtrigger.PredefinedBuildParameters>
              <properties>hw_number=$hw_number
tempest=$tempest
cct_tests=$cct
scenario_name=2a
scenario_job_name=$JOB_NAME
scenario_build_number=$BUILD_NUMBER
</properties>
            </hudson.plugins.parameterizedtrigger.PredefinedBuildParameters>
          </configs>
          <projects>cloud-mkphyscloud-qa-tests-trigger</projects>
          <condition>ALWAYS</condition>
          <triggerWithNoParameters>false</triggerWithNoParameters>
          <triggerFromChildProjects>false</triggerFromChildProjects>
          <block>
            <buildStepFailureThreshold>
              <name>FAILURE</name>
              <ordinal>2</ordinal>
              <color>RED</color>
              <completeBuild>true</completeBuild>
            </buildStepFailureThreshold>
            <unstableThreshold>
              <name>UNSTABLE</name>
              <ordinal>1</ordinal>
              <color>YELLOW</color>
              <completeBuild>true</completeBuild>
            </unstableThreshold>
            <failureThreshold>
              <name>FAILURE</name>
              <ordinal>2</ordinal>
              <color>RED</color>
              <completeBuild>true</completeBuild>
            </failureThreshold>
          </block>
          <buildAllNodesWithLabel>false</buildAllNodesWithLabel>
        </hudson.plugins.parameterizedtrigger.BlockableBuildTriggerConfig>
      </configs>
    </hudson.plugins.parameterizedtrigger.TriggerBuilder>
  </builders>
  <publishers>
    <hudson.tasks.ArtifactArchiver>
      <artifacts>.artifacts/**</artifacts>
      <allowEmptyArchive>true</allowEmptyArchive>
      <onlyIfSuccessful>false</onlyIfSuccessful>
      <fingerprint>false</fingerprint>
      <defaultExcludes>true</defaultExcludes>
      <caseSensitive>true</caseSensitive>
    </hudson.tasks.ArtifactArchiver>
  </publishers>
  <buildWrappers>
    <org.jenkinsci.plugins.buildnamesetter.BuildNameSetter plugin="build-name-setter@">
      <template>#${BUILD_NUMBER} ${ENV,var="cloudsource"} - qa${ENV,var="hw_number"} ${ENV,var="networkingplugin"}:${ENV,var="networkingmode"}</template>
      <runAtStart>true</runAtStart>
      <runAtEnd>true</runAtEnd>
    </org.jenkinsci.plugins.buildnamesetter.BuildNameSetter>
  </buildWrappers>
</project>
